area_id,impact_id,name,name_html,info,info_html,geography
3,0,"The default voices of AI assistants are overwhelmingly female, perpetuating gender roles and stereotypes.","<a href=""https://www.nytimes.com/2019/05/22/world/siri-alexa-ai-gender-bias.html"" target=""_blank"">The default voices of AI assistants are overwhelmingly female, perpetuating gender roles and stereotypes.</a>",Gendered stereotypes result in sexism and can create structural barriers that perpetuate gender inequality.,Gendered stereotypes result in sexism and can create structural barriers that perpetuate gender inequality.,Globally
3,0,"Predictive AI reinforces stereotypes of poor girls and women, stripping autonomy.","Predictive AI reinforces stereotypes of poor girls and women, stripping autonomy.","For example, Microsoft Azure created the machine learning platform for Plataforma Técnologica de Intervención Social in Salta, Argentina, which used bad data (biased, incomplete) for an anti-rights agenda: According to the commissioning party's narratives, if the Ministry have enough information from poor families, conservative public policies can be deployed to predict and avoid abortions by poor women. The public's persistent belief in the infallibility of mathematically-derived algorithms presents another challenge to growing digital rights movements.","For example, Microsoft Azure created the machine learning platform for <a href=""https://www.giswatch.org/node/6203"" target=""_blank"">Plataforma Técnologica de Intervención Soci</a>al in Salta, Argentina, which used bad data (biased, incomplete) for an anti-rights agenda: According to the commissioning party's narratives, if the Ministry have enough information from poor families, conservative public policies can be deployed to predict and avoid abortions by poor women. The public's persistent belief in the infallibility of mathematically-derived algorithms presents another challenge to growing digital rights movements.",Other
3,0,AI tools routinely make sexist assumptions.,AI tools routinely make sexist assumptions.,"For example, the Genderify service assumed a name with the ""Dr."" honorific must be a male name. It also forced 100% of humans into the gender binary with its judgements. The service was widely ridiculed and shut down soon after launching.","For example, the <a href=""https://www.theverge.com/2020/7/29/21346310/ai-service-gender-verification-identification-genderify#:~:text=Genderify%2C%20a%20new%20service%20that,be%20in%20the%20latter%20camp."" target=""_blank"">Genderify service</a> assumed a name with the ""Dr."" honorific must be a male name. It also forced 100% of humans into the gender binary with its judgements. The service was widely ridiculed and shut down soon after launching.",Globally
1,0,Machine learning systems taught with historical data often cement existing inequalities into place.,Machine learning systems taught with historical data often cement existing inequalities into place.,"The result is that those most in need — of a loan, job, university acceptance, or housing — could be the exact groups unfairly locked out based on inaccurate and biased training data.For example, in professional recruiting, matching algorithms that rely on large, unstructured databases risk generating biased profiles of candidates. Certain candidates on gig or recruiting platforms might receive limited employment opportunities or degraded working conditions, thereby perpetuating pre-existing injustices. Litigation and regulatory processes have been slow to reform such algorithmic labor platforms, making it difficult to prove a case and win remuneration against unfair treatment.","The result is that those most in need — of a loan, job, university acceptance, or housing — could be the exact groups unfairly locked out based on inaccurate and biased training data.<br/><br/>For example, in professional recruiting, matching algorithms that rely on large, unstructured databases risk generating biased profiles of candidates. Certain candidates on gig or recruiting platforms might receive limited employment opportunities or degraded working conditions, thereby perpetuating pre-existing injustices. Litigation and regulatory processes have been slow to reform such algorithmic labor platforms, making it difficult to prove a case and win remuneration against unfair treatment.",Globally
0,0,"Discrimination related to ""edge cases.""","Discrimination related to ""edge cases.""","AI design choices that leave a margin of error and allow ""edge cases"" might endanger and discriminate against the communities representing those cases. In 2020, Facebook’s automated content moderation system accidentally flagged posts from Nigerian activists protesting the Special Anti-Robbery Squad (SARS), a controversial police agency that activists say routinely carry out extrajudicial killings against young Nigerians, because the acronym “SARS” was listed by Facebook’s algorithm to be misinformation about the COVID-19 virus.","AI design choices that leave a margin of error and allow ""edge cases"" might endanger and discriminate against the communities representing those cases. In 2020, <a href=""https://slate.com/technology/2020/10/facebook-instagram-endsars-protests-nigeria.html%E2%80%8B."" target=""_blank"">Facebook’s automated content moderation system accidentally flagged posts</a> from Nigerian activists protesting the Special Anti-Robbery Squad (SARS), a controversial police agency that activists say routinely carry out extrajudicial killings against young Nigerians, because the acronym “SARS” was listed by Facebook’s algorithm to be misinformation about the COVID-19 virus.",Globally
3,0,Women are more vulnerable to automation of jobs.,Women are more vulnerable to automation of jobs.,"There's ""a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men,"" according to Dr Cevat Giray Aksoy. In this way, automation threatens to further exacerbate the existing gender pay gap.","There's ""a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men,"" <a href=""https://www.independent.co.uk/news/uk/home-news/robots-gender-pay-gap-uk-automation-work-a9622511.html"" target=""_blank"">according to Dr Cevat Giray Aksoy</a>. In this way, automation threatens to further exacerbate the existing gender pay gap.",Globally
4,0,AI discriminates against people for reasons outside of their control violating human rights.,AI discriminates against people for reasons outside of their control violating human rights.,"AI systems are already discriminating against people for reasons outside of their control, in violation of their inalienable human rights. Whether that harm is predictable, as with a gender-assuming service like Genderify that forces people into a binary based off a poorly-trained learning model, or unexpected, it is unconscionable and, in most jurisdictions, illegal.","AI systems are already discriminating against people for reasons outside of their control, in violation of their inalienable human rights. Whether that harm is predictable, as with a gender-assuming service like <a href=""https://syncedreview.com/2020/07/30/ai-powered-genderify-platform-shut-down-after-bias-based-backlash/"" target=""_blank"">Genderify</a> that forces people into a binary based off a poorly-trained learning model, or unexpected, it is unconscionable and, in most jurisdictions, illegal.",n/a
5,0,Racial bias skews algorithms widely used to guide care from heart surgery to birth.,"<a href=""https://www.statnews.com/2020/06/17/racial-bias-skews-algorithms-widely-used-to-guide-patient-care/"" target=""_blank"">Racial bias skews algorithms widely used to guide care from heart surgery to birth.</a>","A study found that ""the racial bias is created because the algorithm's developers equated higher health care spending with worse health. However, white Americans spend more on healthcare than Black Americans even when their health situation is the same.""","<a href=""https://www.statnews.com/2020/06/17/racial-bias-skews-algorithms-widely-used-to-guide-patient-care/"" target=""_blank"">A study found</a> that ""the racial bias is created because the algorithm's developers equated higher health care spending with worse health. However, white Americans spend more on healthcare than Black Americans even when their health situation is the same.""",United States
5,0,Algorithms are propagating caste-based discrimination on dating apps used by the Hindu community in the UK.,"<a href=""https://velivada.com/2020/02/03/digital-discrimination-is-a-reality-why-shadidotcoms-caste-based-matches-shouldnt-surprise-you/"" target=""_blank"">Algorithms are propagating caste-based discrimination on dating apps used by the Hindu community in the UK</a>.","In February 2020 the Sunday Times reported that ""ShaadiDotCom, one of UK’s biggest marriage sites is reinforcing social divisions by allowing caste discrimination against so-called untouchables, officially known as “scheduled caste.""","In February 2020 the <a href=""https://www.thetimes.co.uk/article/indian-dating-site-deems-untouchables-undateable-08xhkpd6b"" target=""_blank"">Sunday Times reported</a> that ""ShaadiDotCom, one of UK’s biggest marriage sites is reinforcing social divisions by allowing caste discrimination against so-called untouchables, officially known as “scheduled caste.""",United Kingdom
5,0,AI trained to identify hate speech may actually end up amplifying racial bias.,AI trained to identify hate speech may actually end up amplifying racial bias.,"As Vox reported, ""In one study, researchers found that leading AI models for processing hate speech were one-and-a-half times more likely to flag tweets as offensive or hateful when they were written by African Americans, and 2.2 times more likely to flag tweets written in African American English (which is commonly spoken by black people in the US). "" Further investigation showed that ""when moderators knew more about the person tweeting, they were significantly less likely to label that tweet as potentially offensive. At the aggregate level, racial bias against tweets associated with black speech decreased by 11 percent."" (...) ""Another study found similar widespread evidence of racial bias against black speech in five widely used academic data sets for studying hate speech that totaled around 155,800 Twitter posts.""","As <a href=""https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter"" target=""_blank"">Vox reported</a>, ""In <a href=""https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf"" target=""_blank"">one study</a>, researchers found that leading AI models for processing hate speech were one-and-a-half times more likely to flag tweets as offensive or hateful when they were written by African Americans, and 2.2 times more likely to flag tweets written in African American English (which is commonly spoken by black people in the US). "" Further investigation showed that ""when moderators knew more about the person tweeting, they were significantly less likely to label that tweet as potentially offensive. At the aggregate level, racial bias against tweets associated with black speech decreased by 11 percent."" (...) ""<a href=""https://arxiv.org/pdf/1905.12516.pdf"" target=""_blank"">Another study</a> found similar widespread evidence of racial bias against black speech in five widely used academic data sets for studying hate speech that totaled around 155,800 Twitter posts.""",United States
5,0,Facebook’s algorithms have been found to perpetuate race and gender stereotypes that promote financial and job recruiting discrimination.,"<a href=""https://www.theverge.com/2020/7/21/21333405/facebook-instagram-racial-bias-equity-team-formed-ai-algorithms"" target=""_blank"">Facebook’s algorithms have been found to perpetuate race and gender stereotypes that promote financial and job recruiting discriminatio</a>n.","For example, jobs with taxi companies were overwhelmingly delivered to black users and secretarial ads to women, despite neutral targeting preferences.","For example, jobs with taxi companies were overwhelmingly delivered to black users and secretarial ads to women, despite neutral targeting preferences.",Globally
5,0,Biased or incomplete training datasets can result in racial discrimination manifesting via search engine results perpetuating stereotypes and marginalization.,Biased or incomplete training datasets can result in racial discrimination manifesting via search engine results perpetuating stereotypes and marginalization.,"For example, Safiya Umoja Noble has written about how searches for the term “professional hairstyles” in Google returned images of white, blonde women, whereas “unprofessional hairstyles” returned images of Black women. Algorithm Watch has written about machine learning systems at Google and Instagram that consistently assigned negative and harmful labels to images of people with dark skin. For example, the system correctly labels a handheld thermometer when it's held by a light-skinned hand. When a dark-skinned hand holds the same thermometer, the system sees a gun.","For example, Safiya Umoja Noble <a href=""https://nyupress.org/9781479837243/algorithms-of-oppression/"" target=""_blank"">has written</a> about how searches for the term “professional hairstyles” in Google returned images of white, blonde women, whereas “unprofessional hairstyles” returned images of Black women. <a href=""https://algorithmwatch.org/en/story/google-vision-racism/"" target=""_blank"">Algorithm Watch</a> has written about machine learning systems at Google and Instagram that consistently assigned negative and harmful labels to images of people with dark skin. For example, the system correctly labels a handheld thermometer when it's held by a light-skinned hand. When a dark-skinned hand holds the same thermometer, the system sees a gun.",Globally
5,0,Ride-share pricing algorithms charge more for pick-ups originating in non-white areas.,Ride-share pricing algorithms charge more for pick-ups originating in non-white areas.,"Even as Uber and Lyft's policy teams were citing their service of majority-minority neighborhoods in their regulatory battles in cities, their pricing algorithms were charging riders more for pick-ups originating in non-white areas.","Even as Uber and Lyft's policy teams were citing their service of majority-minority neighborhoods in their regulatory battles in cities, their pricing algorithms were <a href=""https://www.newscientist.com/article/2246202-uber-and-lyft-pricing-algorithms-charge-more-in-non-white-areas/"" target=""_blank"">charging riders more</a> for pick-ups originating in non-white areas.",United States
5,0,"Due to a lack of diversity in their training data, common facial recognition systems routinely misidentify Black faces.","Due to a lack of diversity in their training data, common facial recognition systems routinely misidentify Black faces.","The problem was comprehensively described by Joy Buolamwini and Timnit Gebru. In a study by the National Institute of Standards and Technology (NIST), researchers found evidence of racial bias in nearly 200 facial recognition algorithms.","The problem was comprehensively <a href=""http://proceedings.mlr.press/v81/buolamwini18a.html"" target=""_blank"">described by Joy Buolamwini and Timnit Gebru.</a> In a <a href=""https://www.nist.gov/news-events/news/2019/12/nist-study-evaluates-effects-race-age-sex-face-recognition-software"" target=""_blank"">study by the National Institute of Standards and Technology (NIST)</a>, researchers found evidence of racial bias in nearly 200 facial recognition algorithms.",Globally
5,0,UnitedHealth used a discriminatory algorithm for ranking care urgency.,UnitedHealth used a discriminatory algorithm for ranking care urgency.,"New York anti-discrimination regulators probed UnitedHealth for its algorithm after a study found it ranked healthier white patients with the same urgency need as black patients in greater need of care. The algorithm, which used healthcare costs as a poor proxy for need, is sold to hospitals to help determine how to assign additional health-care monitoring resources.","New York <a href=""https://www.wsj.com/articles/new-york-regulator-probes-unitedhealth-algorithm-for-racial-bias-11572087601"" target=""_blank"">anti-discrimination regulators probed UnitedHealth</a> for its algorithm after <a href=""https://www.wsj.com/articles/researchers-find-racial-bias-in-hospital-algorithm-11571941096?mod=article_inline"" target=""_blank"">a study found</a> it ranked healthier white patients with the same urgency need as black patients in greater need of care. The algorithm, which used healthcare costs as a poor proxy for need, is sold to hospitals to help determine how to assign additional health-care monitoring resources.",United States
5,0,"Discriminatory targeted ads focusing on housing, jobs, and credit persist after changes.","<a href=""https://www.theverge.com/2019/3/19/18273018/facebook-housing-ads-jobs-discrimination-settlement"" target=""_blank"">Discriminatory targeted ads focusing on housing, jobs, and credit persist after changes.</a>","Facebook was found to have been allowing advertisers to exclude certain minority groups when advertising within federally regulated markets like housing and jobs. Facebook often sidestepped the concerns using technicalities, such as classifying users not by race but by so-called “ethnic” or “multicultural” affinities, or self-selecting identity groups consistent with what a majority of Black, Hispanic, or other minorities might statistically like on the platform. Only after rigorous coverage from media outlets, most prominently ProPublica, did Facebook eventually disable ad targeting options for housing, job, and credit ads as part of a legal settlement with civil rights groups (NFHA, ACLU, CWA). However, even when Facebook changed its ad platform to prevent advertisers from selecting attributes like “ethnic affinity” for categories like housing or jobs, it was determined that the platform still enabled discrimination by allowing advertisers to target users through proxy attributes.","Facebook was found to have been allowing advertisers to exclude certain minority groups when advertising within federally regulated markets like housing and jobs. Facebook often sidestepped the concerns using technicalities, such as classifying users not by race but by so-called “ethnic” or “multicultural” affinities, or self-selecting identity groups consistent with what a majority of Black, Hispanic, or other minorities might statistically like on the platform. Only after rigorous coverage from media outlets, <a href=""https://www.propublica.org/article/facebook-ads-can-still-discriminate-against-women-and-older-workers-despite-a-civil-rights-settlement"" target=""_blank"">most prominently ProPublica</a>, did Facebook eventually <a href=""https://about.fb.com/news/2019/03/protecting-against-discrimination-in-ads/"" target=""_blank"">disable </a>ad targeting options for housing, job, and credit ads as part of a legal settlement with civil rights groups (<a href=""https://nationalfairhousing.org/"" target=""_blank"">NFHA</a>, <a href=""https://www.aclu.org/blog/womens-rights/womens-rights-workplace/facebook-settles-civil-rights-cases-making-sweeping"" target=""_blank"">ACLU</a>, <a href=""https://cwa-union.org/news/releases/class-action-lawsuit-hits-tmobile-amazon-cox-for-alleged-age-discrimination"" target=""_blank"">CWA</a>). However, even when Facebook changed its ad platform to prevent advertisers from selecting attributes like “ethnic affinity” for categories like housing or jobs, it was determined that the platform still enabled discrimination by allowing advertisers to<a href=""https://hal.archives-ouvertes.fr/hal-01955343"" target=""_blank""> target users through proxy attributes</a>.",United States
5,0,Clearview AI appears to have been designed with explicitly racist use cases in mind.,"Clearview AI appears to have been designed with <a href=""https://medium.com/@AINowInstitute/ai-and-the-far-right-a-history-we-cant-ignore-f81375c3cc57"" target=""_blank"">explicitly racist use cases in mind</a>.","According to the Huffington Post report, Chuck Johnson - a former Breitbart writer - posted in January 2017 that he was involved in “building algorithms to ID all the illegal immigrants for the deportation squads.” Clearview AI has now signed a paid contract with Immigration and Customs Enforcement, which is using predictive analytics and facial recognition software to accelerate the detention and deportation of undocumented people in the United States, even as the pandemic unfolds around us. It is in use by some 2,400 police departments in the US.","According to the Huffington Post report, Chuck Johnson - a former Breitbart writer - posted in January 2017 that he was involved in “building algorithms to ID all the illegal immigrants for the deportation squads.” Clearview AI has now signed a paid contract with Immigration and Customs Enforcement, which is using predictive analytics and facial recognition software to accelerate the detention and deportation of undocumented people in the United States, even as the pandemic unfolds around us. It is in use by some <a href=""https://www.theverge.com/2021/1/10/22223349/clearview-ai-facial-recognition-law-enforcement-capitol-rioters"" target=""_blank"">2,400 police departments in the US</a>.",United States
5,0,A test-proctoring app reinforces structural oppression.,A test-proctoring app reinforces structural oppression.,"Proctorio, a test problematic test-proctoring app, reinforces structural oppression through bad-tech microaggressions, such as asking a Black woman to shine more light on her face for the system to recognize her.","Proctorio, a test problematic test-proctoring app, reinforces structural oppression through bad-tech microaggressions, such as <a href=""https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/?utm_medium=tr_social&amp;utm_campaign=site_visitor.unpaid.engagement&amp;utm_source=Twitter#Echobox=1596778938"" target=""_blank"">asking a Black woman to shine more light on her face</a> for the system to recognize her.",United States
5,0,Algorithms used to assess kidney function or predict heart failure use race as a central criterion.,Algorithms used to assess kidney function or predict heart failure use race as a central criterion.,"There is no scientific basis to do so, and the results discriminate against Black people, who are assigned less points automatically and therefore less likely to receive medical treatment.","There is no scientific basis to do so, <a href=""https://algorithmwatch.ch/en/racial-health-bias/"" target=""_blank"">and the results discriminate against Black</a> people, who are assigned less points automatically and therefore less likely to receive medical treatment.",Other
4,1,Predictive AI can mistakenly out LGBTQI+ people.,Predictive AI can mistakenly out LGBTQI+ people.,"A Stanford University study demonstrated an AI model that can accurately guess whether people are gay or straight based on their faces, and sparked a swift backlash from LGBTQI+ rights activists who fear this kind of technology could be used to harm queer people.","<a href=""https://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford"" target=""_blank"">A Stanford University</a> study demonstrated an AI model that can accurately guess whether people are gay or straight based on their faces, and sparked a swift backlash from LGBTQI+ rights activists who fear this kind of technology could be used to harm queer people.",n/a
4,1,Ignoring complexity of gender identities.,Ignoring complexity of gender identities.,"The Transgender Rights movement has had to push back against technological systems that force people into gender binaries. From Facebook's ""sex"" field to AI systems like Genderify that force definitions of people into male / female binaries by their name, face, or other traits, machine decision making systems have routinely ignored the more complicated realities of being human when they don't fit neatly into poorly conceived computer systems.","The <a href=""https://en.wikipedia.org/wiki/Transgender_rights_movement"" target=""_blank"">Transgender Rights</a> movement has had to push back against technological systems that force people into gender binaries. From Facebook's ""sex"" field to AI systems like Genderify that force definitions of people into male / female binaries by their name, face, or other traits, machine decision making systems have routinely ignored the more complicated realities of being human when they don't fit neatly into poorly conceived computer systems.",n/a
4,2,AI and robotics will become integral to future security systems and therefore to border control and preventing irregular migration.,"AI and robotics will become integral to <a href=""http://www.mixedmigration.org/articles/artificial-intelligence-and-radical-technical-innovation-the-impact-of-the-fourth-industrial-revolution-on-mixed-migration/"" target=""_blank"">future security systems and therefore to border control and preventing irregular migration</a>.","Mixed Migration Review explains that the likely impact of demographic changes and environmental stressors amongst other future changes will lead the number of migrants and refugees in the world to increase significantly. With asylum seekers, refugees and migrants, travelling together in mixed irregular flows, often in vulnerable situations, this raises serious concerns.Migrants and refugees have been called some of ""the most surveilled people on the planet,"" as they cross multiple national borders where government immigration authorities collect and compute copious amounts of new data sources (such as social media data) through black box algorithms offered by vendors to automate what can, unfortunately, amount to life-or-death decisions.","Mixed Migration Review explains that the likely impact of demographic changes and environmental stressors amongst other future changes will lead the number of migrants and refugees in the world to increase significantly. With asylum seekers, refugees and migrants, travelling together in mixed irregular flows, often in vulnerable situations, this raises serious concerns.<br/><br/>Migrants and refugees have been called some of ""the most surveilled people on the planet,"" as they cross multiple national borders where government <a href=""https://privacyinternational.org/protecting-migrants-borders-and-beyond"" target=""_blank"">immigration authorities</a> collect and compute copious amounts of new data sources (such as social media data) through black box algorithms offered by vendors to automate what can, unfortunately, amount to life-or-death decisions.",Globally
5,3,Amazon's algorithm used to determine speed delivery options discriminated against minority neighbourhoods.,Amazon's algorithm used to determine speed delivery options discriminated against minority neighbourhoods.,"The predictive algorithm Amazon used to determine which parts of a city could enjoy same-day Prime Delivery discriminated against minority urban neighborhoods. The company excluded the entirety of the Bronx, for example. The distinct disparity in service caused political backlash when Bloomberg News uncovered it, and Amazon has since overridden its algorithm to extend same-day delivery in Boston, Chicago and New York.","The predictive algorithm Amazon used to determine which parts of a city could enjoy <a href=""https://www.bloomberg.com/graphics/2016-amazon-same-day/"" target=""_blank"">same-day Prime Delivery</a> discriminated against minority urban neighborhoods. The company excluded the entirety of the Bronx, for example. The distinct disparity in service caused political backlash when Bloomberg News uncovered it, and Amazon has since overridden its algorithm to extend same-day delivery in Boston, Chicago and New York.",n/a
3,3,Driving financial discrimination against the marginalized in the Global South.,"<a href=""https://www.ilo.org/wcmsp5/groups/public/---dgreports/---cabinet/documents/publication/wcms_647306.pdf"" target=""_blank"">Driving financial discrimination against the marginalized in the Global South.</a>","The job-killing ""creative destruction"" expected to follow incoming waves of AI and robotics advancements is likely to inflict the most damage on the exact countries that are least able to nimbly respond. At minimum, emerging economies face a torrid period of short- and medium-term pain as they adjust to a post-AI economy. It is not even clear what this adjustment might look like — how can these countries improve their position in a world without jobs for their millions of low-wage workers.","The job-killing ""creative destruction"" expected to follow incoming waves of AI and robotics advancements is likely to inflict the most damage on the exact countries that are least able to nimbly respond. At minimum, emerging economies face a torrid period of short- and medium-term pain as they adjust to a post-AI economy. It is not even clear what this adjustment might look like — how can these countries improve their position in a world without jobs for their millions of low-wage workers.",Global South
3,3,Economic vulnerability for working class women in tech supply chains.,Economic vulnerability for working class women in tech supply chains.,"There are approximately 190 million women working in global supply chains. While their jobs may offer women some economic independence, if social standards go unmanaged, the reality can include low wages, excessive hours, unsafe conditions, and sexual harassment. AI's promise to eliminate labor could eliminate this hard-earned economic freedom and exacerbate poor working conditions. Experts have predicted that AI will hollow out the middle of the labor force, so that only very high skilled and low skilled jobs remain.","There are approximately <a href=""https://www.ethicaltrade.org/issues/gender-equity-global-supply-chains"" target=""_blank"">190 million women working in global supply chains</a>. While their jobs may offer women some economic independence, if social standards go unmanaged, the reality can include low wages, excessive hours, unsafe conditions, and sexual harassment. AI's promise to eliminate labor could eliminate this hard-earned economic freedom and exacerbate poor working conditions. Experts have predicted that AI will hollow out the middle of the labor force, so that only very high skilled and low skilled jobs remain.",Globally
3,3,Facebook's ad algorithm has been found to stereotype potential job applicants by race and gender.,Facebook's ad algorithm has been found to stereotype potential job applicants by race and gender.,"A study by researchers at Northeastern University found that jobs with taxi companies were overwhelmingly delivered to black users and secretarial ads to women, despite neutral targeting preferences by the people running the ads.","<a href=""https://arxiv.org/abs/1904.02095"" target=""_blank"">A study by researchers at Northeastern University</a> found that jobs with taxi companies were overwhelmingly delivered to black users and secretarial ads to women, despite neutral targeting preferences by the people running the ads.",United States
3,3,"AI has been found to exacerbate bias in recruitment, negatively impacting women, and especially women of color.","AI has been found to exacerbate bias in recruitment, negatively impacting women, and especially women of color.","Women at the Table's Artificial Intelligence Recruitment study dives into the bias in AI applications for Human Resources recruiting. In 2014, for example, Amazon found that an AI algorithm it developed to automate headhunting taught itself to bias against female candidates.","<a href=""https://www.womenatthetable.net/"" target=""_blank"">Women at the Table's Artificial Intelligence Recruitment study</a> dives into the bias in AI applications for Human Resources recruiting. In 2014, for example, Amazon found that <a href=""https://slate.com/business/2018/10/amazon-artificial-intelligence-hiring-discrimination-women.html"" target=""_blank"">an AI algorithm it developed to automate headhunting</a> taught itself to bias against female candidates.",n/a
1,3,"Growing automation of agricultural work might cause less work for farm workers, predominantly women in Asia/Africa/other geographies.","<a href=""http://documents1.worldbank.org/curated/en/777731585054424384/pdf/The-Future-of-Work-in-Agriculture-Some-Reflections.pdf"" target=""_blank"">Growing automation of agricultural work might cause less work for farm workers, predominantly women in Asia/Africa/other geographies.</a>","Robots already do much of the harvesting of lettuce and tomatoes in some greenhouses in the global north. All these developments have had a significant impact on the labour market. For countries in the industrial world, this growing automation probably means the continued decline of rural life. In Africa and large parts of Asia, women in rural areas constitute the majority of the agricultural labour force in small-scale and subsistence farming. Since official statistics do not capture unpaid work, be it in the garden, in the field or in the household, they insufficiently represent women’s actual share in agricultural work. Women in Africa and Asia who live in rural areas are often doubly affected by discrimination.","Robots already do much of the harvesting of lettuce and tomatoes in some greenhouses in the global north. All these developments have had a significant impact on the labour market. For countries in the industrial world, this growing automation probably means the continued decline of rural life. In Africa and large parts of Asia, women in rural areas constitute the majority of the agricultural labour force in small-scale and subsistence farming. Since official statistics do not capture unpaid work, be it in the garden, in the field or in the household, they insufficiently represent women’s actual share in agricultural work. Women in Africa and Asia who live in rural areas are often doubly affected by discrimination.",Globally
1,3,The 'ghost work' of AI is done by low-paid workers across developing countries.,"<a href=""https://www.forbes.com/sites/adigaskell/2019/09/02/the-ghost-workers-powering-the-ai-economy/?sh=5a0cbc6143eb"" target=""_blank"">The 'ghost work' of AI is done by low-paid workers across developing countries.</a>","The “hidden ghost work” of data cleaning, image labelling, text processing and content moderation being performed by back-end workers across developing economies. Recently, much has been written about the wage differentials in digital labour and the evidently meagre remuneration being paid to surveilled global South workers doing the “janitorial” labour that keeps digital platforms healthy and productive. Not only this, it has also come to light that many AI assistants are in fact fully powered by real human agents working in developing countries. When massive data sets are used to train AI systems, the individual images and videos involved are commonly tagged and labeled. There are millions of low-paid tech workers doing this backend kind of work.","The “hidden ghost work” of data cleaning, image labelling, text processing and content moderation being performed by back-end workers across developing economies. Recently, much has been written about the wage differentials in digital labour and the evidently meagre remuneration being paid to surveilled global South workers doing the “janitorial” labour that keeps digital platforms healthy and productive. Not only this, it has also come to light that many AI assistants are in fact fully powered by real human agents working in developing countries. When massive data sets are used to train AI systems, the individual images and videos involved are commonly tagged and labeled. There are millions of low-paid tech workers doing this backend kind of work.",Global South
1,3,"The majority of the workers, yet the most vulnerable: there are approximately 190 million women working in global supply chains.","<a href=""https://www.ethicaltrade.org/issues/gender-equity-global-supply-chains"" target=""_blank"">The majority of the workers, yet the most vulnerable: there are approximately 190 million women working in global supply chains.</a>","While their jobs may offer women some economic independence, if social standards go unmanaged, the reality can include low wages, excessive hours, unsafe conditions, and sexual harassment. Women are more likely to be affected by significant economic shifts, such as COVID-19 contractions and AI-uptake.","While their jobs may offer women some economic independence, if social standards go unmanaged, the reality can include low wages, excessive hours, unsafe conditions, and sexual harassment. Women are more likely to be affected by significant economic shifts, such as COVID-19 contractions and AI-uptake.",Globally
1,3,In countries where the price of labour is very low and education outcomes are also low the uptake of AI will most likely be slowest.,"In countries where the price of labour is very low and education outcomes are also low <a href=""http://www.mixedmigration.org/articles/artificial-intelligence-and-radical-technical-innovation-the-impact-of-the-fourth-industrial-revolution-on-mixed-migration/"" target=""_blank"">the uptake of AI will most likely be slowest.</a>","The Government Artificial Intelligence Readiness Index illustrates that many refugee and migrant countries of origin are ill-equipped to make changes towards AI.  For example, Somalia, Eritrea, Sudan, South Sudan, and the Democratic Republic of  Congo are all countries that produce both migrants and refugees, are at the very bottom (last 10) of the index, and also in the lowest decile of most human development rankings. By contrast the most preferred countries of destination for migrants and refugees are those with the highest AI readiness ranking.The ILO takes data from many countries around the world, so it provides a comprehensive picture. The Government Artificial Intelligence Readiness Index shows that the countries that are least ready to make changes towards AI are: Somalia, Eritrea, Sudan, South Sudan, Democratic Republic of Congo. In general, they are countries with the lowest human development rankings, which means that all of these countries should be prioritized more in thinking about uptake of AI/ethical AI/trustworthy AI.","The Government Artificial Intelligence Readiness Index illustrates that many refugee and migrant countries of origin are ill-equipped to make changes towards AI. For example, Somalia, Eritrea, Sudan, South Sudan, and the Democratic Republic of Congo are all countries that produce both migrants and refugees, are at the very bottom (last 10) of the index, and also in the lowest decile of most human development rankings. By contrast the most preferred countries of destination for migrants and refugees are those with the highest AI readiness ranking.<a href=""https://content.sciendo.com/view/journals/izajolp/9/1/article-20190004.xml?language=en#j_izajolp-2019-0004_ref_061_w2aab3b8c46b1b7b1ab2b2c61Aa"" target=""_blank"">The ILO takes data from many countries around the world, so it provides a comprehensive picture.</a> The Government Artificial Intelligence Readiness Index shows that the countries that are least ready to make changes towards AI are: Somalia, Eritrea, Sudan, South Sudan, Democratic Republic of Congo. In general, they are countries with the lowest human development rankings, which means that all of these countries should be prioritized more in thinking about uptake of AI/ethical AI/trustworthy AI.",Globally
1,3,AI might worsen the hollowing out of the middle class and widening of socioeconomic inequality by affecting manufacturing and demand for labor.,"<a href=""https://content.sciendo.com/view/journals/izajolp/9/1/article-20190004.xml?language=en#j_izajolp-2019-0004_ref_061_w2aab3b8c46b1b7b1ab2b2c61Aa"" target=""_blank"">AI might worsen the hollowing out of the middle class and widening of socioeconomic inequality by affecting manufacturing and demand for labor</a>.","The current wave of AI-driven technological change comes at a time when the anticipated benefits from the previous wave of digitization have not been even distributed and where costs – in the form of historic rates of inequality and low income growth for the middle class – are manifesting. Society has been through cycles of creative destruction before, where new technology makes certain forms of labor redundant, but concerns are rising that this time, increases in unemployment and drops in earnings might be permanent. In periods of stagnating output, increases in labor productivity induced by new technologies necessarily lead to a fall in labor demand. Even if AI does not lead to fewer jobs, such shifts could cause working conditions to deteriorate and earnings to fall further behind productivity, as they already have in the past. Women, ethnic and racial minorities, rural, and lower-income individuals will be disproportionately affected by such shifts. Women are overrepresented in vulnerable and informal employment sectors in low and lower-middle income countries, and they systematically lack social protections including sick pay, maternity pay, parental leave or the opportunity to unionize.","The current wave of AI-driven technological change comes at a time when the anticipated benefits from the previous wave of digitization have not been even distributed and where costs – in the form of historic rates of inequality and low income growth for the middle class – are manifesting. Society has been through cycles of creative destruction before, where new technology makes certain forms of labor redundant, but concerns are rising that this time, increases in unemployment and drops in earnings might be permanent. In periods of stagnating output, increases in labor productivity induced by new technologies necessarily lead to a fall in labor demand. <br/><br/>Even if AI does not lead to fewer jobs, such shifts could cause working conditions to deteriorate and earnings to fall further behind productivity, as they already have in the past. Women, ethnic and racial minorities, rural, and lower-income individuals will be disproportionately affected by such shifts. Women are overrepresented in vulnerable and informal employment sectors in low and lower-middle income countries, and they systematically lack social protections including sick pay, maternity pay, parental leave or the opportunity to unionize.",Globally
5,3,Racial discrimination in consumer mortgage lending and housing.,Racial discrimination in consumer mortgage lending and housing.,"AI could amplify discrimination against racial minorities in consumer mortgage lending and housing, found a study at UC Berkeley.","AI could amplify discrimination against racial minorities in consumer mortgage lending and housing, <a href=""http://faculty.haas.berkeley.edu/morse/research/papers/discrim.pdf"" target=""_blank"">found a study at UC Berkeley.</a>",United States
2,4,AI is intensifying energy consumption.,AI is intensifying energy consumption.,"The environmental movement originally saw tech and digital transformation as an efficiency for a resource-depleted planet. The dream of ""the paperless office"" has given way to the reality of planned obsolescence and e-waste and huge carbon footprints of the server farms that power cloud computing, encryption methods like Bitcoin, and AI models.Research suggests that AI is intensifying energy consumption, especially from the development of AI by major tech companies: Amazon, Microsoft, and Google. Currently, there is little to no information about how much energy big tech’s algorithms consume, but data suggest that the biggest carbon emissions are coming from training models and the storing of large datasets and a training a single AI model can emit as much carbon as five cars in their lifetime. The ad tech industry is assumed to be the biggest pollutant in this area.","The environmental movement originally saw tech and digital transformation as an efficiency for a resource-depleted planet. The dream of ""the paperless office"" has given way to the reality of planned obsolescence and e-waste and huge carbon footprints of the server farms that power cloud computing, encryption methods like Bitcoin, and AI models.Research suggests that AI is intensifying energy consumption, especially from the development of AI by major tech companies: Amazon, Microsoft, and Google. Currently, there is little to no information about how much energy big tech’s algorithms consume, but <a href=""https://medium.com/@AINowInstitute/ai-and-climate-change-how-theyre-connected-and-what-we-c%20an-do-about-it-6aa8d0f5b32c"" target=""_blank"">data suggest that the biggest carbon emissions are coming from training models </a>and the storing of large datasets and a <a href=""https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/"" target=""_blank"">training a single AI model can emit as much carbon as five cars in their lifetime.</a> The ad tech industry is assumed to be the biggest pollutant in this area.","Research on energy emissions/impact of AI is from the Global North, but the disproportionate impacts of energy consumption and climate change is almost always in the Global South."
2,5,AI is accelerating fossil fuel extraction.,AI is accelerating fossil fuel extraction.,"The AI supply chain is powered by extractive industries, and the mining of materials needed for computational devices. AI is also used for extractive purposes — for example, AI is being used to pinpoint the exact location for water extraction, which is further privatizing water management. In addition, a Greenpace report exposes how the three largest cloud companies — Amazon (33% market share), Microsoft (18%), and Google (8%) — are partnering with oil companies to use artificial intelligence technologies to accelerate the unlocking of oil and gas deposits in the US and around the world at a time where we cannot afford any additional carbon in the atmosphere.","The <a href=""https://anatomyof.ai/"" target=""_blank"">AI supply chain is powered by extractive industries, and the mining of materials needed for computational devices</a>. AI is also used for extractive purposes — for example, AI is being used to pinpoint the exact location for water extraction, which is further privatizing water management. In addition, a <a href=""https://www.greenpeace.org/usa/reports/oil-in-the-cloud/"" target=""_blank"">Greenpace report exposes</a> how the three largest cloud companies — Amazon (33% market share), Microsoft (18%), and Google (8%) — are partnering with oil companies to use artificial intelligence technologies to accelerate the unlocking of oil and gas deposits in the US and around the world at a time where we cannot afford any additional carbon in the atmosphere.",Globally
1,6,Tech workers who speak up risk retaliation.,Tech workers who speak up risk retaliation.,"As described in Mozilla's ""Trustworthy AI"" white paper: When tech workers do decide to organize and speak out about their companies’ business decisions, they run the risk of retaliation. At Google, Amazon, and Wayfair, tech workers have been fired or penalized for protesting their companies’ contracts with US Immigration & Customs Enforcement (ICE).","As described in Mozilla's <a href=""https://foundation.mozilla.org/en/insights/trustworthy-ai-whitepaper/ai-challenges/"" target=""_blank"">""Trustworthy AI"" white paper:</a> When tech workers do decide to organize and speak out about their companies’ business decisions, they run the risk of retaliation. At Google, Amazon, and Wayfair, tech workers have been fired or penalized for protesting their companies’ contracts with US Immigration & Customs Enforcement (ICE).",United States
1,6,The tech workers who perform the invisible maintenance of AI are particularly vulnerable to exploitation and overwork.,"The tech workers who perform the invisible maintenance of AI<a href=""https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona"" target=""_blank""> are particularly vulnerable to exploitation and overwork.</a>","Companies building AI-powered services rely on a vast network of on-demand workers to clean and label datasets, and to train and improve models. Workers who perform content moderation for platforms like Facebook and Twitter, for instance, are regularly subjected to disturbing imagery, sounds, and language, suffering serious mental health problems and secondhand trauma as a result.","Companies building AI-powered services rely on a vast network of on-demand workers to clean and label datasets, and to train and improve models. Workers who perform content moderation for platforms like Facebook and Twitter, for instance, are regularly subjected to disturbing imagery, sounds, and language, suffering serious mental health problems and secondhand trauma as a result.",United States
1,6,Employers are increasingly turning to surveillance technologies to monitor remote workers' activity on electronic devices and measure their productivity.,"Employers are increasingly turning to <a href=""https://news.trust.org/item/20210107111527-x235l/"" target=""_blank"">surveillance technologies to monitor remote workers' activity on electronic devices and measure their productivity.</a>","Microsoft promoting its own productivity monitoring tools late last year, that was the clearest sign this was becoming very mainstream. (N.B.: In December, after labour groups like Prospect criticized productivity measuring tools in Microsoft 365 software, the company made changes to bolster privacy in the suite of apps, such as giving managers visibility into aggregated data rather than individual activity.","Microsoft promoting its own productivity monitoring tools late last year, that was the clearest sign this was becoming very mainstream. (N.B.: In December, after labour groups like Prospect criticized productivity measuring tools in Microsoft 365 software, the company made changes to bolster privacy in the suite of apps, such as giving managers visibility into aggregated data rather than individual activity.",Globally
1,6,Algorithmic gig workers management might violate their rights.,Algorithmic gig workers management might violate their rights.,"Few employment laws around the world adequately reflect the realities of the gig economy and its reliance on algorithmic management. This form of labor is often precarious and temporary, with few benefits and little support. Further, the workers don't have access to their own data or the possibility to influence or verify how the algorithm works in managing them, while the pandemic exposes them to increased risks and surveillance. A recent Italian court ruling found that Deliveroo's algorithmic management of its gig workers violated Italian labor law by punishing workers for exercising their labor rights, such as staying home when sick. In the 2020 election, California voters rejected workers' bid to be considered legal employees of platforms like Lyft and Uber, leaving them in a precarious contractor status.","Few employment laws around the world adequately reflect the realities of the gig economy and its reliance on algorithmic management. This form of labor is often <a href=""https://www.aljazeera.com/economy/2020/7/1/brazil-deliverymen-for-uber-other-apps-protest-amid-pandemic"" target=""_blank"">precarious and temporary</a>, with few benefits and little support. Further, the workers don't have access to their own data or the possibility to influence or verify how the algorithm works in managing them, while the pandemic exposes them <a href=""https://www.adalovelaceinstitute.org/blog/covid-19-gig-economy-hunger-for-surveillance/"" target=""_blank"">to increased risks and surveillance.</a> A recent <a href=""https://twitter.com/IvanaBartoletti/status/1345992299323531264?s=20"" target=""_blank"">Italian court ruling found that Deliveroo's algorithmic management of its gig workers violated Italian labor law</a> by punishing workers for exercising their labor rights, such as staying home when sick. In the 2020 election, California voters <a href=""https://www.nytimes.com/2020/11/04/technology/california-uber-lyft-prop-22.html"" target=""_blank"">rejected workers' bid</a> to be considered legal employees of platforms like Lyft and Uber, leaving them in a precarious contractor status.",Globally
1,6,Companies are training untested and infantilizing automated oversight technology on blue collar workers.,Companies are training untested and infantilizing automated oversight technology on blue collar workers.,"Both Amazon and Essity, a Swedish toilet paper manufacturer operating in France, have saddled warehouse employees with proximity alarms they must wear as they work (the Amazon system involves an AI-based wearable camera). The alarms shriek if they spend too much time in proximity of other workers, in the name of reducing the spread of COVID. The French workers are taking the issue up with their union, whereas Amazon's workers are not allowed to opt out.","Both Amazon and Essity, a Swedish toilet paper manufacturer operating in France, have saddled warehouse employees with proximity alarms they must wear as they work (the Amazon system involves an AI-based wearable camera). The alarms shriek if they spend too much time in proximity of other workers, in the name of reducing the spread of COVID. The French workers are <a href=""https://feeds.thelocal.com/app/android/beta/article.php?e=french-workers-furious-at-social-distancing-alarms-in-workplace"" target=""_blank"">taking the issue up with their union</a>, whereas <a href=""https://www.theverge.com/2020/6/16/21292669/social-distancing-amazon-ai-assistant-warehouses-covid-19"" target=""_blank"">Amazon's workers</a> are not allowed to opt out.",Global North
0,7,Majority of extremist group joins on Facebook are due to its recommendation tools.,Majority of extremist group joins on Facebook are due to its recommendation tools.,"An internal Facebook researcher found that “64% of all extremist group joins are due to our recommendation tools”. Facebook's own algorithmic group recommendation products, “Groups You Should Join” and “Discover”, ""grow the problem.” The company has now announced it will no longer recommend political groups with these tools, although previous experience suggests there will be omissions.","<a href=""https://www.wsj.com/articles/facebook-knows-it-encourages-division-top-executives-nixed-solutions-11590507499"" target=""_blank"">An internal Facebook researcher</a> found that “64% of all extremist group joins are due to our recommendation tools”. Facebook's own algorithmic group recommendation products, “Groups You Should Join” and “Discover”, ""grow the problem.” The company <a href=""https://www.reuters.com/article/us-facebook-groups-idUSKBN29X00C"" target=""_blank"">has now announced</a> it will no longer recommend political groups with these tools, although previous experience suggests there will be omissions.",Globally
0,7,AI tech founders with far-right connections.,AI tech founders with far-right connections.,"AI Now Institute discusses the troubling connections between prominent AI tech founders and far-right extremists, which might influence both how AI is developed and deployed at a time when the growing visibility of the far-right has been identified as a major threat to democracies.","AI Now Institute discusses the troubling connections between <a href=""https://medium.com/@AINowInstitute/ai-and-the-far-right-a-history-we-cant-ignore-f81375c3cc57"" target=""_blank"">prominent AI tech founders and far-right extremists</a>, which might influence both how AI is developed and deployed at a time when the growing visibility of the <a href=""https://icct.nl/app/uploads/2020/04/The-growing-power-of-online-communities-of-the-extreme-right.pdf"" target=""_blank"">far-right has been identified as a major threat to democracies.</a>",Globally
0,7,Hate speech is finding new forums in decentralized spaces.,Hate speech is finding new forums in decentralized spaces.,"While it has been often pointed out that YouTube's recommendation engine might push its viewers towards more extreme content addressing major platforms harms might not be enough. As described by Mozilla fellow Emmi Beevensee, even when pushed off of major platforms such as Twitter and YouTube, hate speech is finding new forums in decentralized spaces that are often not yet prepared to deal with it, but that still help these actors reach mass audiences. The resiliency of online communications is proving a hurdle for disrupting the organizing of far-right violence.","While it has been often <a href=""https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html"" target=""_blank"">pointed out that YouTube's recommendation engine might push its viewers towards more extreme content</a> addressing major platforms harms might not be enough. As described by Mozilla fellow Emmi Beevensee,<a href=""https://foundation.mozilla.org/en/blog/fellow-research-decentralized-web-hate/"" target=""_blank""> even when pushed off of major platforms such as Twitter and YouTube, hate speech is finding new f</a>orums in decentralized spaces that are often not yet prepared to deal with it, but that still help these actors reach mass audiences. The resiliency of online communications is proving a hurdle for disrupting the organizing of far-right violence.",Globally
0,7,Algorithmic online advertising strategies might spread far-right messaging and radicalization.,"<a href=""https://qz.com/1677549/how-the-far-right-tricks-online-shoppers-into-spreading-its-message/"" target=""_blank"">Algorithmic online advertising strategies might spread far-right messaging and radicalization</a>.","This can happen by marketing hate merchandise and brands that can act as literal and ideological gateways to more and more extreme content. Online retail platforms like Amazon routinely fail to moderate their marketplaces, and respond only reactively when media coverage points out offending materials for sale.","This can happen by marketing hate merchandise and brands that <a href=""https://press.princeton.edu/books/hardcover/9780691170206/the-extreme-gone-mainstream"" target=""_blank"">can act as literal and ideological gateways to more and more extreme content.</a> Online retail platforms like Amazon routinely fail to moderate their marketplaces, and respond only reactively when <a href=""https://www.vox.com/recode/2021/1/11/22224722/amazon-oath-keepers-3-three-percenters-hats-badges-merch"" target=""_blank"">media coverage</a> points out offending materials for sale.<br/>",United States
3,8,Online violence against women in politics.,"<a href=""https://carnegieendowment.org/2020/11/30/tackling-online-abuse-and-disinformation-targeting-women-in-politics-pub-83331"" target=""_blank"">Online violence against women in politics</a>.","Women in politics around the world are frequent targets of abuse and threats online, but social media companies and governments are not doing nearly enough to combat it. Lucina Di Meco and Saskia Brechenmacher break down the issue and what can be done to tackle it. Some amount of this harassment is computationally generated with AI bots and tools, and this issue has significant overlap with content moderation dialogues.","Women in politics around the world are frequent targets of abuse and threats online, but social media companies and governments are not doing nearly enough to combat it. Lucina Di Meco and Saskia Brechenmacher break down the issue and what can be done to tackle it. Some amount of this harassment is computationally generated with AI bots and tools, and this issue has significant overlap with content moderation dialogues.",Globally
3,8,"AI-generated fake videos (Deepfakes) are being used to harass, intimidate, demean, undermine, and destabilize women.","AI-generated fake videos (Deepfakes) are being used to harass, intimidate, demean, undermine, and destabilize women.","They are becoming more common (and convincing) and women are being disproportionately affected by them, as seen through deepfake porn. Sensity AI's research reveals deepfake videos are growing rapidly online in 2019, doubling over the last seven months to 14,678, with 96% concerning reputation attacks to public figures in the form of falsified adult material. Websites considered in the analysis received over 134 million views, and the most common victims' country of origin are the US, the UK, South Korea, Canada, and India. Another Sensity AI report outlines an investigation into an underground deepfake bot embedded in Telegram chatrooms was used to synthesize fake naked photos of approximately 104,000 women, using their social media profile pictures without their knowledge. An estimated 70% of the victims are private individuals. Their evidence indicates that the bot had over 100,000 users overall, 70% of whom appeared to be Russian speaking.","They are becoming more common (and convincing) and women are being disproportionately affected by them, as seen through deepfake porn. <a href=""https://sensity.ai/reports/"" target=""_blank"">Sensity AI's research</a> reveals deepfake videos are growing rapidly online in 2019, doubling over the last seven months to 14,678, with <a href=""https://sensity.ai/mapping-the-deepfake-landscape/"" target=""_blank"">96% concerning reputation attacks to public figures in the form of falsified adult material.</a> Websites considered in the analysis received over 134 million views, and the most common victims' country of origin are the US, the UK, South Korea, Canada, and India. Another Sensity AI report outlines an investigation into an underground deepfake bot embedded in Telegram chatrooms was used to synthesize fake naked photos of approximately 104,000 women, using their social media profile pictures without their knowledge. An estimated 70% of the victims are private individuals. Their evidence indicates that the bot had over 100,000 users overall, 70% of whom appeared to be Russian speaking.",Globally
4,9,"The web and AI should be equally accessible to all people, regardless of disability or status.","The web and AI should be equally accessible to all people, regardless of disability or status.",,,n/a
3,10,Data bias due to lack of data on women in the healthcare industry.,Data bias due to lack of data on women in the healthcare industry.,"In Invisible Women, Caroline Criado Perez shares research on the ""gender data gap,"" or paucity of data about women in health and safety. The result is that the healthcare industry is optimized for men, to the detriment and poor health outcomes for non-men. Even more concerning is the potential for new AI-enabled tools to ""learn"" from this data and further increase social inequalities.","In <a href=""https://www.penguin.co.uk/books/111/1113605/invisible-women/9781784706289.html"" target=""_blank"">Invisible Women</a>, Caroline Criado Perez shares research on the ""gender data gap,"" or paucity of data about women in health and safety. The result is that the healthcare industry is optimized for men, to the detriment and poor health outcomes for non-men. Even more concerning is the potential for new AI-enabled tools to ""learn"" from this data and further increase social inequalities.",Globally
2,10,"There’s a need for a better understanding of environmental harms and AI among policy-makers, experts, and the broader public.","There’s a need for a better understanding of environmental harms and AI among policy-makers, experts, and the broader public.","A workshop on ""Trustworthy AI x Climate Crisis"" workshop concluded that ‘Experts’ feel unequipped to talk about this intersection. This is a new field, and there is a general sense that none has the ‘answer.’ Participants expressed a desire to  define the problem and gain a clear understanding of the environmental harms of AI. They wanted to articulate the connections between AI and the climate crisis that addresses underlying power structures and might yield possible solutions. There's also a need for a clear articulation of AI harms on which environments are most likely to be harmed. The analyses should include who and which environments are most likely to be harmed, and a clear understanding of which harms we are talking about, from AI supply chain, carbon emissions, to how it is optimizing the global extraction economy.","A <a href=""https://wiki.mozilla.org/Projects/Sustainability/Workshops"" target=""_blank"">workshop on ""Trustworthy AI x Climate Crisis""</a> workshop concluded that ‘Experts’ feel unequipped to talk about this intersection. This is a new field, and there is a general sense that none has the ‘answer.’ Participants expressed a desire to define the problem and gain a clear understanding of the environmental harms of AI. They wanted to articulate the connections between AI and the climate crisis that addresses underlying power structures and might yield possible solutions. There's also a need for a clear articulation of AI harms on which environments are most likely to be harmed. The analyses should include who and which environments are most likely to be harmed, and a clear understanding of which harms we are talking about, from AI supply chain, carbon emissions, to how it is optimizing the global extraction economy.","Assumed that most important environmental impacts of AI will be felt in Global South/poorer/developing countries, as most impacts of climate crisis/environmental injustice are already felt there."
5,10,"Most US states collapse ethnicity into race in their reporting, obscuring uneven outcomes across distinct communities.","Most US states collapse ethnicity into race in their reporting, obscuring <a href=""https://covidtracking.com/blog/tracking-race-and-ethnicity"" target=""_blank"">uneven outcomes across distinct communities</a>.","There are well-known disparities between the social and economic outcomes of White and Black Hispanic groups.The same issue arises with corporate diversity data. The US form companies are required to use (EEO-1) asks them to match their employees to a limited subset of racial categories, with only one ethnicity category for ""Hispanic."" This makes tracking the diversity of tech companies relative to the overall population a difficult task. To accurately assess disparities in representation and outcomes, we need complete demographic data that treats race and ethnicity as separate but related data layers that can be used to pinpoint community needs.","There are well-known disparities between the social and economic outcomes of White and Black Hispanic groups.The same issue arises with corporate diversity data. The US form companies are required to use (EEO-1) asks them to match their employees to a limited subset of racial categories, with only one ethnicity category for ""Hispanic."" This makes tracking the diversity of tech companies relative to the overall population a difficult task. To accurately assess disparities in representation and outcomes, we need complete demographic data that treats race and ethnicity as separate but related data layers that can be used to pinpoint community needs.",United States
5,10,Lack of data on race/ethnicity leads to biased AI.,Lack of data on race/ethnicity leads to biased AI.,"In the context of race and racism, data helps validate experiences and perspectives that are often undercut, misunderstood, silenced or ignored. It took until 2020 for Facebook to dedicate internal teams to study its main social network and Instagram for racial bias, and in particular, whether its algorithms trained using artificial intelligence adversely affect Black, Hispanic, and other underrepresented groups.","In the context of race and racism, data helps validate experiences and perspectives that are often undercut, misunderstood, silenced or ignored. <a href=""https://www.theverge.com/2020/7/21/21333405/facebook-instagram-racial-bias-equity-team-formed-ai-algorithms"" target=""_blank"">It took until 2020 for Facebook to dedicate internal teams to study its main social network and Instagram for racial bias</a>, and in particular, whether its algorithms trained using artificial intelligence adversely affect Black, Hispanic, and other underrepresented groups.",United States
0,11,Bad actors gaming AI systems to distort results.,Bad actors gaming AI systems to distort results.,"AI systems are prone to gaming by motivated actors, who can distort the results and gain more attention to their venomous message. For example, Google Search’s autocomplete suggestions ""have been hijacked by malicious users to display antisemitic, sexist, and racist language.""","AI systems are prone to gaming by motivated actors, who can distort the results and gain more attention to their venomous message. For example, Google Search’s autocomplete suggestions <a href=""https://foundation.mozilla.org/en/insights/trustworthy-ai-whitepaper/ai-challenges/"" target=""_blank"">""have been hijacked by malicious users to display antisemitic, sexist, and racist language.""</a>",United States
1,11,Funding for AI fueled mental health apps is surging while the field is lacking enough research to be safe.,Funding for AI fueled mental health apps is surging while the field is lacking enough research to be safe.,"A bevy of startups have raised tens of millions of dollars in funding to apply AI and chatbots to provide mental health services to address everything from insomnia to anxiety and depression. This raises immediate ethical concerns around privacy, plus the evidence supporting the use of AI for mental health hasn't yet been established: one scientific review of 73 available mental health apps found only one referenced published scientific literature to support its claims.","A bevy of startups have raised tens of millions of dollars in funding to apply AI and chatbots to provide mental health services to address everything from insomnia to anxiety and depression. This raises immediate <a href=""https://www.statnews.com/2019/08/07/5-questions-mental-health-tech/"" target=""_blank"">ethical concerns</a> around privacy, plus the evidence supporting the use of AI for mental health hasn't yet been established: one <a href=""https://www.nature.com/articles/s41746-019-0093-1"" target=""_blank"">scientific review</a> of 73 available mental health apps found only one referenced published scientific literature to support its claims.",United States
1,11,"Coercive consumerism: the entire Enlightenment concept of free will may be a thing of the past as our autonomy in consumer choice is eroded in the context of algorithms, Big Tech, and targeted ads.","<a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7278769/"" target=""_blank"">Coercive consumerism:</a> the entire Enlightenment concept of free will may be a thing of the past as our autonomy in consumer choice is eroded in the context of algorithms, Big Tech, and targeted ads.","The use of our data to manipulate us into purchasing something we don't need is a new privacy concern. With big data, the possibilities of shaping people’s behaviour have become ever more comprehensive.Today’s online advertising ecosystem runs on data. Companies are tracking you across the web and gathering information that helps them determine which ads are most likely to get you to purchase a product if shown at the right time. Our decisions are influenced by far more forces than we might realize.","The use of our data to manipulate us into purchasing something we don't need is a new privacy concern. With big data, the possibilities of shaping people’s behaviour have become ever more comprehensive.Today’s online advertising ecosystem runs on data. Companies are tracking you across the web and gathering information that helps them determine which ads are most likely to get you to purchase a product if shown at the right time. Our decisions are influenced by far more forces than we might realize.",Globally
1,12,"Big tech accumulates data with acquisitions, hindering competition.","Big tech accumulates data with acquisitions, hindering competition.","Concentration at the top of the tech industry continues unabated, making meaningful competition nearly impossible. Google's proposed acquisition of Fitbit is emblematic of consolidation amongst a small handful of dominant monopoly platforms. The acquisition would give Google even more health data about millions of people. The acquisition has passed review of US and EU competition authorities. The Australian Competition & Consumer Commission, however, rejected Google's proposal due to its likely effect on non-Apple wearable manufacturers, and will rule on whether it will allow the merger by March 25, 2021.","Concentration at the top of the tech industry continues unabated, making meaningful competition nearly impossible. Google's proposed acquisition of Fitbit is emblematic of consolidation amongst a small handful of dominant monopoly platforms. The acquisition would give Google even more health data about millions of people. The acquisition has passed review of US and EU competition authorities. <a href=""https://www.accc.gov.au/media-release/accc-rejects-google-behavioural-undertakings-for-fitbit-acquisition"" target=""_blank"">The Australian Competition &amp; Consumer Commission</a>, however, rejected Google's proposal due to its likely effect on non-Apple wearable manufacturers, and will rule on whether it will allow the merger by March 25, 2021.",Globally
0,13,AI opacity may affect societal issues like election or school determination.,AI opacity may affect societal issues like election or school determination.,"AI systems are commonly the opposite of transparent. Their developers keep their inputs and methods secret due to competition, to prevent gaming of the algorithm (as with Google Search), and often, because they themselves aren't entirely sure how the AI comes up with the outcomes it does. This is a concern for commercial products, but becomes even more problematic when those products affect societal issues. For example, The Markup found that Gmail's black box algorithm controls which political emails arrived in subscribers' main inboxes. Different 2020 presidential candidates had starkly different rates of successful deliverability.The opacity of AI systems is also a troubling issue when the AI is employed by government to determine which school your child goes to, or whether you qualify for social benefits. Major resource allocation decisions like this need to be auditable and accountable, and secret algorithms don't meet those standards.","AI systems are commonly the opposite of transparent. Their developers keep their inputs and methods secret due to competition, to prevent gaming of the algorithm (as with Google Search), and often, because they themselves aren't entirely sure how the AI comes up with the outcomes it does. This is a concern for commercial products, but becomes even more problematic when those products affect societal issues. For example, <a href=""https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email"" target=""_blank"">The Markup</a> found that Gmail's black box algorithm controls which political emails arrived in subscribers' main inboxes. Different 2020 presidential candidates had starkly different rates of successful deliverability.<br/><br/>The opacity of AI systems is also a troubling issue when the AI is employed by government to determine which school your child goes to, or whether you qualify for social benefits. Major resource allocation decisions like this need to be auditable and accountable, and secret algorithms don't meet those standards.",United States
0,14,AI might be deployed to help in the fight for social change.,AI might be deployed to help in the fight for social change.,Social movements don't just suffer from AI's harms — they also employ it toward their own missions. Pro-social AI projects leverage emerging tech to boost their work on existing challenges.,Social movements don't just suffer from AI's harms — they also employ it toward their own missions. Pro-social AI projects leverage emerging tech to boost their work on existing challenges.,n/a
3,15,The creators of deepfakes particularly target and disproportionately impact/sexualize women and girls.,The creators of deepfakes particularly target and disproportionately impact/sexualize women and girls.,"96% of deepfakes in the English-speaking internet are non-consensual adult images of women, according to a Sensity AI 2019 deepfake mapping scan.","96% of deepfakes in the English-speaking internet are non-consensual adult images of women, according to a Sensity AI <a href=""https://sensity.ai/mapping-the-deepfake-landscape/"" target=""_blank"">2019 deepfake mapping scan</a>.",n/a
3,15,AI development in sex robots has the potential for manipulation and coercion.,AI development in sex robots has the potential for manipulation and coercion.,"""AI development in sex robots requires immediate policy attention, presents special risks and opportunities to develop manipulative/coercive AI. Advanced machine learning may allow robots to cultivate love and devotion, the ability to elicit personal information or to manipulate and influence behavior. These capabilities are all theoretically possible, and perhaps more importantly, they are profitable for AI sex robots to cultivate. With the worldwide sex technology reportedly worth 30 billion USD (Kleeman, 2017), the market may incentivize the development of AI capabilities that may be vastly more consequential than blinking silicon sex dolls."" ""Sex Robots — A Harbinger for Emerging AI Risk."" See also: Sexbots: The Ethical Ramifications of Social Robotics’ Dark Side.","""<a href=""https://www.theguardian.com/world/2017/sep/08/ai-gay-gaydar-algorithm-facial-recognition-criticism-stanford"" target=""_blank"">AI development in sex robots</a> requires immediate policy attention, presents special risks and opportunities to develop manipulative/coercive AI. Advanced machine learning may allow robots to cultivate love and devotion, the ability to elicit personal information or to manipulate and influence behavior. These capabilities are all theoretically possible, and perhaps more importantly, they are profitable for AI sex robots to cultivate. With the worldwide sex technology reportedly worth 30 billion USD (Kleeman, 2017), the market may incentivize the development of AI capabilities that may be vastly more consequential than blinking silicon sex dolls."" ""Sex Robots — A Harbinger for Emerging AI Risk."" See also: <a href=""https://sigai.acm.org/static/aimatters/3-4/AIMatters-3-4-11-Wagner.pdf"" target=""_blank"">Sexbots: The Ethical Ramifications of Social Robotics’ Dark Side</a>.",n/a
3,15,Bias search engines return gender discriminating results.,Bias search engines return gender discriminating results.,"Dr. Safiya U. Noble's Algorithms of Oppression: How Search Engines Reinforce Racism exposed how biased search engines would return results for queries like “black girls” with vulgar, sexualized content. It was an early and powerful example of the issues with relying on machine (Google's search algorithm) understanding of datasets.","Dr. Safiya U. Noble's <a href=""https://en.wikipedia.org/wiki/Algorithms_of_Oppression#:~:text=Algorithms%20of%20Oppression%3A%20How%20Search,%2C%20and%20human%2Dcomputer%20interaction."" target=""_blank"">Algorithms of Oppression: How Search Engines Reinforce Racism</a> exposed how biased search engines would return results for <a href=""https://time.com/5209144/google-search-engine-algorithm-bias-racism/"" target=""_blank"">queries like “black girls”</a> with vulgar, sexualized content. It was an early and powerful example of the issues with relying on machine (Google's search algorithm) understanding of datasets.",n/a
5,16,Sao Paolo Metro System deployed a facial recognition system to monitor reactions to ads.,Sao Paolo Metro System deployed a facial recognition system to monitor reactions to ads.,"The Sao Paolo Metro system setup, without warning, a facial recognition system to monitor mass transit users' reactions to advertising, and put out a bid for instituting facial recognition across the transportation network. In both cases, consumer rights group IDEC sued.","<a href=""https://idec.org.br/noticia/idec-e-internetlab-lancam-guia-para-uso-responsavel-do-reconhecimento-facial"" target=""_blank"">The Sao Paolo Metro system</a> setup, without warning, a facial recognition system to monitor mass transit users' reactions to advertising, and put out a bid for instituting facial recognition across the transportation network. In both cases, consumer rights group IDEC sued.",Brazil
5,16,US law enforcement uses flawed facial recognition tech.,US law enforcement uses flawed facial recognition tech.,"Thousands of police departments in the US regularly employ fundamentally flawed facial recognition technology, leading to a self-fulfilling prophecy of broken criminal justice systems.","Thousands of police departments in the US r<a href=""https://www.vox.com/recode/2019/12/10/20996085/ai-facial-recognition-police-law-enforcement-regulation"" target=""_blank"">egularly employ fundamentally flawed facial recognition technology</a>, leading to a self-fulfilling prophecy of broken criminal justice systems.",United States
5,16,Brazil banned the use of cameras for emotions recognition in public and outlawed sale of biometric data.,Brazil banned the use of cameras for emotions recognition in public and outlawed sale of biometric data.,"In Brazil, the Personal Data Protection Bill specifically outlaws the use of cameras to collect and infer emotional state in public spaces, and the sale of biometric data to third parties.","In Brazil, the <a href=""https://idec.org.br/noticia/veja-10-coisas-que-mudarao-na-sua-vida-com-lei-de-dados-pessoais"" target=""_blank"">Personal Data Protection Bill</a> specifically outlaws the use of cameras to collect and infer emotional state in public spaces, and the sale of biometric data to third parties.",Brazil
1,17,Consumer watchdogs class action against misuse of social media user data.,Consumer watchdogs class action against misuse of social media user data.,"The Cambridge Analytica scandal inspired consumer watchdog groups in Italy, Belgium, Spain, and Portugal to file class action lawsuits against Facebook for selling access to their fellow citizens' attention based on their user data. Facebook has thus far successfully delayed action in the lawsuits by challenging the national courts' ability to rule on the matter. Italy's Altro Consumo consumer group is demanding the company reimburse its Italian Facebook users 285 Euro per year on the platform for the misuse of their data.","The Cambridge Analytica scandal inspired consumer watchdog groups in <a href=""https://www.altroconsumo.it/azioni-collettive/facebook"" target=""_blank"">Italy</a>, <a href=""https://mlexmarketinsight.com/insights-center/editors-picks/area-of-expertise/data-privacy-and-security/facebooks-snarling-of-european-data-breach-lawsuits-shows-limits-of-private-enforcement"" target=""_blank"">Belgium, Spain, and Portugal</a> to file class action lawsuits against Facebook for selling access to their fellow citizens' attention based on their user data. Facebook has thus far successfully delayed action in the lawsuits by challenging the national courts' ability to rule on the matter. Italy's Altro Consumo consumer group is demanding the company reimburse its Italian Facebook users <a href=""https://www.altroconsumo.it/azioni-collettive/facebook"" target=""_blank"">285 Euro per year on the platform</a> for the misuse of their data.",European Union
4,18,"The Chinese tech ecosystem is racing to innovate and patent AI-powered methods to detect, track, and surveill Uighurs, a persecuted minority group.","The Chinese tech ecosystem is racing to innovate and <a href=""https://news.trust.org/item/20210113195157-jq6lj/"" target=""_blank"">patent AI-powered methods</a> to detect, track, and surveill Uighurs, a persecuted minority group.","China is converting the entire region where Uighurs live into a ""massive internment camp,"" according to the UN. AI tools used to track them include facial recognition to distinguish their features vs. majority groups, and predictive policing tools to help determine who to detain.","China is converting the entire region where Uighurs live into a ""massive internment camp,"" according to the UN. AI tools used to track them include facial recognition to distinguish their features vs. majority groups, and predictive policing tools to help determine who to detain.",Other
4,18,State actors and others can readily repurpose AI to violate human rights.,State actors and others can readily repurpose AI to violate human rights.,"The same AI tech that powers convenient features for consumers can be used by bad actors toward horrific ends. It was discovered that MIT-funded iFlytek technology was sold to the Chinese government and used to surveill and oppress ethnic Uighurs in China's northwest, where China has forced the ethnic and religious minority into camps. MIT cut ties with the company only after a broader ban on certain foreign funding took effect. The incident demonstrates how state actors and others can readily repurpose AI to violate human rights, and the international relations already intertwined with the development of AI.","The same AI tech that powers convenient features for consumers can be used by bad actors toward horrific ends. It was discovered that <a href=""https://www.wired.com/story/mit-cuts-ties-chinese-ai-firm-human-rights/"" target=""_blank"">MIT-funded iFlytek technology</a> was sold to the Chinese government and used to surveill and oppress ethnic Uighurs in China's northwest, where <a href=""https://www.voanews.com/east-asia-pacific/voa-news-china/un-39-countries-condemn-chinas-abuses-uighurs"" target=""_blank"">China has forced the ethnic and religious minority into camps</a>. MIT cut ties with the company only after a broader ban on certain foreign funding took effect. The incident demonstrates how state actors and others can readily repurpose AI to violate human rights, and the international relations already intertwined with the development of AI.",n/a
3,19,Women are more likely to be targets of doxxing and cybermobbing.,Women are more likely to be targets of doxxing and cybermobbing.,"As with deepfakes, women are far more likely to be the targets of much of the doxxing, stalking, and cybermobbing that takes place online. Coordinated groups violate targets' privacy and then share their data in ways meant to intimidate, humiliate, and threaten. They often employ advanced techniques to hack this personal data, as with the password cracking that led to the exposure of many celebrities' personal iPhone photo galleries.","As with deepfakes, women are <a href=""https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119429128.iegmc009"" target=""_blank"">far more likely</a> to be the targets of much of the doxxing, stalking, and cybermobbing that takes place online. Coordinated groups violate targets' privacy and then share their data in ways meant to intimidate, humiliate, and threaten. They often employ advanced techniques to hack this personal data, as with the password cracking that led to the exposure of many celebrities' personal iPhone photo galleries.",n/a
4,19,Companies obtaining AI training data by violating privacy or manipulation.,Companies obtaining AI training data by violating privacy or manipulation.,"Basic consumer rights are violated when companies obtain personal data without respecting privacy, or through manipulative default settings and Terms of Service agreements written in an inaccessible manner. A particularly glaring example of obtaining data without respecting privacy occurred when, in 2019, Google contractors were found to be targeting homeless Black people in their efforts to capture images to further train its facial recognition research.","Basic consumer rights are violated when companies obtain personal data without respecting privacy, or through manipulative default settings and Terms of Service agreements written in an inaccessible manner. A particularly glaring example of obtaining data without respecting privacy occurred when, <a href=""https://www.nytimes.com/2019/10/04/technology/google-facial-recognition-atlanta-homeless.html"" target=""_blank"">in 2019, Google contractors were found to be targeting homeless Black people in their efforts to capture images to further train its facial recognition research</a>.",Globally
5,19,Commercial facial recognition companies drive surveillance in cities.,Commercial facial recognition companies drive surveillance in cities.,"Black people in the US are already over-surveilled and harassed by law enforcement at rates many times white peoples' experiences. AI is likely to make this problem worse as cities embed biased, inaccurate facial recognition models into surveillance cameras. Banjo AI signed a multi-million dollar contract with Utah to run its crime detection algorithms atop the state's many CCTV feeds. The problematic-in-oh-so-many-ways Clearview AI is going a step further and developing its own surveillance camera. The result is likely to be yet more surveillance and violation of non-white citizens' privacy.","Black people in the US are already over-surveilled and harassed by law enforcement at rates many times white peoples' experiences. AI is likely to make this problem worse as cities embed biased, inaccurate facial recognition models into surveillance cameras. <a href=""https://www.engadget.com/2020-03-04-banjo-ai-utah-law-enforcement-surveillance.html"" target=""_blank"">Banjo AI</a> signed a multi-million dollar contract with Utah to run its crime detection algorithms atop the state's many CCTV feeds. The problematic-in-oh-so-many-ways <a href=""https://www.buzzfeednews.com/article/carolinehaskins1/clearview-facial-recognition-insight-camera-glasses"" target=""_blank"">Clearview AI</a> is going a step further and developing its own surveillance camera. The result is likely to be yet more surveillance and violation of non-white citizens' privacy.",United States
5,20,Google's open source tool meant to solve hate speech ended up amplifying racist stereotypes.,Google's open source tool meant to solve hate speech ended up amplifying racist stereotypes.,"Even as racial minorities suffer from racism, stereotyping, and hate speech leveled against them in their daily lives, AI's dependence on biased data looks to make matters worse. Jigsaw's (a company of Alphabet) Perspective API (used by news site such as the New York Times), meant to solve toxic online conversations, instead labeled conversations in African-American Vernacular as offensive hate speech. Rather than protect victims of racism from hate speech, Jigsaw's service accused them of it.","Even as racial minorities suffer from racism, stereotyping, and hate speech leveled against them in their daily lives, AI's dependence on biased data looks to make matters worse. Jigsaw's (a company of Alphabet) <a href=""https://www.perspectiveapi.com/#/home"" target=""_blank"">Perspective API</a> (used by news site such as the New York Times), meant to solve toxic online conversations, instead <a href=""https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf"" target=""_blank"">labeled conversations in African-American Vernacular</a> as offensive hate speech. Rather than protect victims of racism from hate speech, Jigsaw's service accused them of it.",United States
1,21,The global supply chain of AI replicates colonial inequities.,The global supply chain of AI replicates colonial inequities.,"The global supply chain of AI (data, computational power, natural resources, labor) today replicates historical colonial inequities, and the continued subordination of Global South countries and extraction of wealth to enrichen the Global North. If AI is as powerful a technological revolution as many believe, it will only widen the already gaping chasm between rich and poor countries worldwide.","The global supply chain of AI (data, computational power, natural resources, labor) today <a href=""https://dl.acm.org/doi/10.1145/3375627.3375859"" target=""_blank"">replicates historical colonial inequities, and the continued subordination of Global South countries</a> and extraction of wealth to enrichen the Global North. If AI is as powerful a technological revolution as many believe, it will only widen the already gaping chasm between rich and poor countries worldwide.",Globally
